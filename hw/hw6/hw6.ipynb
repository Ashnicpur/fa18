{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f02089e1eccc4756",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Homework 6: Scalable Data Processing Using Ray\n",
    "Contributors: Peter Schafhalter, Robert Nishihara, Edward Fang, Simon Mo, Devin Petersohn\n",
    "\n",
    "![ray_logo](https://i.imgur.com/6DHvEJil.jpg)\n",
    "\n",
    "## Due Date: Friday 12/7, 11:59PM\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "Here are some important course policies. These are also located at\n",
    "http://www.ds100.org/fa18/.\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your solution.\n",
    "\n",
    "## This Assignment\n",
    "Although you've learned powerful techniques for processing data, most of the problems you've worked on involved small datasets that a single machine can quickly process. Data science in practical settings often involves processing hundreds of gigabytes, terabytes, and even petabytes of data. \n",
    "\n",
    "These data sets are too large for a single computer to process quickly, so data scientists often *scale up* their programs to *clusters*, many computers working together to run a program. On a smaller scale, we can also parallelize programs across processor cores in a single computer.\n",
    "\n",
    "For this homework, we will use [Ray](https://ray.readthedocs.io/en/latest/), a system for parallel and distributed Python (developed at Berkeley), which can parallelize programs across the cores of a single computer as well as a cluster. While your code for this homework will only parallelize across the cores of a single JupyterHub computer, you could run it on a cluster as well without any changes.\n",
    "\n",
    "Throughout this assignment, you may find it helpful to refer to the following:\n",
    "- [Ray documentation](https://ray.readthedocs.io/en/latest/)\n",
    "- [Ray codebase](https://github.com/ray-project/ray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96c3cc1eaad03e69",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 1: Learning the Ray API\n",
    "\n",
    "Let's start by digging into the Ray API and familiarize ourselves with some of its functionalities. Again, please reference the [Ray documentation](https://ray.readthedocs.io/en/latest/) and [Ray codebase](https://github.com/ray-project/ray) if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1ef66fc29a67901",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.spatial\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import timeit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f8dd14785cef69b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# To start Ray, we will first import ray and call `ray.init`. \n",
    "# We will use 4 cpus. \n",
    "\n",
    "# Note that you should only run this cell once!\n",
    "import ray\n",
    "ray.init(include_webui=False, num_cpus=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7045bc2ef431553a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "You may have noticed Ray print a line that looks similar to the following:\n",
    "> `Starting local scheduler with the following resources: {'CPU': 4, 'GPU': 0}.`\n",
    "\n",
    "This means that we have 4 CPU cores available. In theory, we can parallelize programs to run 4x as fast!\n",
    "\n",
    "For now, let's start with the basics. With Ray, Python objects can live either within process or within Ray's shared memory which transfers objects between processes in order to share data across different functions that might run in parallel.\n",
    "\n",
    "`ray.put(x)` copies an object `x` from the current process to Ray's shared object store and returns an ID for that object.\n",
    "\n",
    "`ray.get(x_id)` returns the object determined by the object's ID `x_id` from Ray's shared object store.\n",
    "\n",
    "You can think of this as sending python object to a in-memory database so it can be accessed by other peer python processes. \n",
    "\n",
    "If that seems a little confusing, that's ok! The following example should clear it up. The most important thing is to understand `ObjectID`s and `ray.get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-82ff4103df5d3985",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# x is a python object within this process.\n",
    "x = 42  \n",
    "print(f\"x's type is {type(x)} and its value is {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef9ad94a88be868b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Copy x to Ray's object store. \n",
    "# This lets you share x among different Ray worker processes.\n",
    "x_id = ray.put(x)  \n",
    "print(f\"x_id's type is {type(x_id)} and its value is {x_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-451658536f69adb4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Copy the object determined by x_id from Ray's object store to this process.\n",
    "x_copy = ray.get(x_id)  \n",
    "print(f\"x_copy's type is {type(x_copy)} and its value is {x_copy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-83aac10013fb7d57",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "You can pass a list of `ObjectID`s to `ray.get` in order to retrieve a list of objects stored in Ray's object store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2d1ec8c9fa27c46",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_id = ray.put(49)  # Store 49 in Ray's object store.\n",
    "x_copy, y_copy = ray.get([x_id, y_id])  # Retrieve a list of ObjectIDs from Ray's object store.\n",
    "\n",
    "print(f\"x_copy's type is {type(x_copy)} and its value is {x_copy}\")\n",
    "print(f\"y_copy's type is {type(y_copy)} and its value is {y_copy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1a\n",
    "Store `a`, `b`, and `c` in Ray's object store and retrieve their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "a = \"Go bears!\"\n",
    "b = [i for i in range(10)]\n",
    "c = {\"Berkeley\": \"#1\", \"Stanford\": \"#2\"}\n",
    "\n",
    "a_id = ...\n",
    "b_id = ...\n",
    "c_id = ...\n",
    "\n",
    "a_copy = ...\n",
    "b_copy = ...\n",
    "c_copy = ...\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "a_id = ray.put(a)\n",
    "b_id = ray.put(b)\n",
    "c_id = ray.put(c)\n",
    "\n",
    "a_copy = ray.get(a_id)\n",
    "b_copy, c_copy = ray.get([b_id, c_id])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# they should have the same value!\n",
    "assert a_copy == a\n",
    "assert b_copy == b\n",
    "assert c_copy == c\n",
    "\n",
    "# they shouldn't be the same object\n",
    "assert a_copy is not a\n",
    "assert b_copy is not b\n",
    "assert c_copy is not c\n",
    "\n",
    "# is the id really object id?\n",
    "assert isinstance(a_id, type(x_id))\n",
    "assert isinstance(b_id, type(x_id))\n",
    "assert isinstance(c_id, type(x_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a07fd7c817630544",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's try to parallelize a function using Ray. Simply add the `@ray.remote` decorator to the function you wish to parallelize.\n",
    "\n",
    "Note that using the decorator as below is [syntactic sugar](https://en.wikipedia.org/wiki/Syntactic_sugar) for `square_ray = ray.remote(square_ray)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8296ac91e75af87",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "@ray.remote\n",
    "def square_ray(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca0ec4f91f6f4305",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We'll call functions that we wish to parallelize *remote functions* because we intend to run them remotely in a different process instead of in the current process (the remote process could be on the same machine or on a different machine). A [*process*](https://en.wikipedia.org/wiki/Process_(computing)) consists of program code and activity. Programs, including those written with Ray, may consist of multiple processes which allow them to execute code in parallel.\n",
    "\n",
    "Remote functions are invoked with the `.remote()` method. You can pass function arguments to the `.remote` method which will immediately return an `ObjectID` for the return value of the function and launch a *task* that executes the function. This means that Ray returns an `ObjectID` repersenting the result of a function before the function finishes exceuting which can be useful if you'd like a long-running function to execute in the background while running other computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de3098b37ac11615",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "result_id = square_ray.remote(42)\n",
    "print(f\"result_id's type is {type(result_id)} and its value is {result_id}\")\n",
    "\n",
    "result = ray.get(result_id)\n",
    "print(f\"result's type is {type(result)} and its value is {result}\")\n",
    "print(f\"42**2 = {square(42)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1b\n",
    "Create a new function `slow_function_ray` using the @ray.remote decorator to turn `slow_function` into a remote function. Then call the remote function and get its result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# This function is a proxy for a more interesting and computationally intensive function.\n",
    "def slow_function():\n",
    "    time.sleep(0.2)\n",
    "    return 42\n",
    "\n",
    "# use a decorator here\n",
    "def slow_function_ray():\n",
    "    ...\n",
    "\n",
    "result = ...\n",
    "### BEGIN SOLUTION\n",
    "@ray.remote\n",
    "def slow_function_ray():\n",
    "    time.sleep(0.2)\n",
    "    return 42\n",
    "result = ray.get(slow_function_ray.remote())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1b-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert result == slow_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1c\n",
    "Now let's verify that remote functions are actually running in parallel. `slow_function` takes around 0.2 seconds to run. If we execute 2 slow functions sequentially, executing both should take $ 2 \\times 0.2 = 0.4 $ seconds. However, if we run 2 slow functions in parallel, executing both should still take only 0.2 seconds.\n",
    "\n",
    "Let's try it out! Modify the code below so that calling slow_function twice takes only 0.2 seconds to run.\n",
    "\n",
    "**HINT:** use `slow_function_ray` and the fact that calling a remote function immediately returns an `ObjectID` even if the remote function isn't done executing yet. You might need to split the code into 2 lines that:\n",
    "1. Calls the remote functions and returns `ObjectID`s\n",
    "2. Gets the results of the remote functions from the `ObjectID`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "# YOUR CODE HERE: Make the following line execute in 0.2 seconds using Ray\n",
    "# results = [slow_function() for _ in range(2)]\n",
    "\n",
    "result_ids = ...\n",
    "results = ...\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "result_ids = [slow_function_ray.remote() for _ in range(2)]\n",
    "results = ray.get(result_ids)\n",
    "### END SOLUTION\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(\"Time to compute results: {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1c-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert results == [42, 42]\n",
    "assert np.isclose(end_time - start_time, 0.2, rtol=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc541ff3d6bf4d30",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Arguments to remote functions can either be regular Python objects or `ObjectID`s. Passing `ObjectID`s as arguments can be useful for calling remote functions on the results of other remote functions. This allows you to initiate tasks that depend on other tasks before any of the tasks complete.\n",
    "\n",
    "Therefore, the following is equivalent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-304048d399698d94",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Call square_ray on a python object\n",
    "result1_id = square_ray.remote(42)\n",
    "\n",
    "# Call square_ray on an ObjectID\n",
    "arg_id = ray.put(42)\n",
    "result2_id = square_ray.remote(arg_id)\n",
    "\n",
    "result1, result2 = ray.get([result1_id, result2_id])\n",
    "\n",
    "print(\"{} == {}\".format(result1, result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [*task*](https://en.wikipedia.org/wiki/Task_%28computing%29) is a loosely-defined term referring to a unit of work. In the context of Ray, tasks correspond to code which Ray executes. Calling a remote function corresponds to creating a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1d\n",
    "Let's compute $5^8$ by applying `square_ray` 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Implement the following using square_ray.remote with only 1 call to ray.get\n",
    "\n",
    "# result = 5\n",
    "# for _ in range(3):\n",
    "#     result = square(result)\n",
    "\n",
    "result = ...\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "x = 5\n",
    "for _ in range(3):\n",
    "    x = square_ray.remote(x)\n",
    "\n",
    "result = ray.get(x)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1d-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert result == 5**8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c92b47e2234f69e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 2: Parallel Bootstrap\n",
    "\n",
    "In lab 10, you learned how to use bootstrap to estimate mean and variance. Now, let's parallelize bootstrap using Ray in order to speed up bootstrap.\n",
    "\n",
    "Below we have the solution for how to implement `simple_resample` and `boostrap_serial`, they should look familiar to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-343b1992231109c8",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_resample(n):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n: an integer\n",
    "        \n",
    "    Returns:\n",
    "        an array of length n of a random sample with replacement of\n",
    "        the integers 0, 1, ..., n-1\n",
    "    \"\"\"\n",
    "    return(np.random.randint(low=0, high=n, size=n))\n",
    "\n",
    "def bootstrap_serial(boot_pop, statistic, resample, replicates = 1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        boot_pop: an array of shape n x d.\n",
    "        statistic: a function which takes boot_pop and returns a number.\n",
    "        resample: a function which takes n and returns a random sample from the integers [0, n)\n",
    "        replicates: the number of resamples\n",
    "        \n",
    "    Returns:\n",
    "        an array of length replicates, each entry being the statistic computed on a bootstrap sample of the data.\n",
    "    \"\"\"\n",
    "    n = len(boot_pop)\n",
    "    resample_estimates = np.array([statistic(boot_pop[resample(n)]) for _ in range(replicates)])\n",
    "    return resample_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f43fcc4e4cf2881",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2a\n",
    "\n",
    "Use the @ray.remote decorator along with the `bootstrap_serial` function defined above to write a `bootstrap_remote` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0c1fc2071fa622c",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: turn `bootstrap_remote` into a remote function\n",
    "### BEGIN SOLUTION\n",
    "@ray.remote\n",
    "### END SOLUTION  \n",
    "def bootstrap_remote(boot_pop, statistic, resample, replicates = 1000):\n",
    "    \"\"\"Run bootstrap_serial remotely\n",
    "    Args:\n",
    "        boot_pop: an array of shape n x d.\n",
    "        statistic: a function which takes boot_pop and returns a number.\n",
    "        resample: a function which takes n and returns a random sample from the integers [0, n)\n",
    "        replicates: the number of resamples\n",
    "        \n",
    "    Returns:\n",
    "        an array of length replicates, each entry being the statistic computed on a bootstrap sample of the data.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return bootstrap_serial(boot_pop, statistic, resample, replicates)\n",
    "    ### END SOLUTION    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"grades_sample.csv\")\n",
    "boot_pop = np.array(data[\"Grade\"])\n",
    "num_bootstrap_resample = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7d09ce6105a8903",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "boot_sample_means_serial = bootstrap_serial(boot_pop, np.mean, simple_resample, num_bootstrap_resample)\n",
    "bootstrap_serial_time = time.perf_counter() - start_time\n",
    "print(f\"Bootstrap serial completed in {bootstrap_serial_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4bcb5098798c5f34",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, implement bootstrap in parallel. \n",
    "- First, create 10 tasks by calling `bootstrap_remote` which each generate `num_bootstrap_resample // 10` resamples.\n",
    "- Then, get the results of the tasks and merge them into a single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b741497e36ef5a4e",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "NUM_TASKS = 10\n",
    "\n",
    "boot_sample_means_ids = ...\n",
    "boot_sample_means_parallel = ...\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "boot_sample_means_ids = [bootstrap_remote.remote(boot_pop, np.mean, simple_resample, num_bootstrap_resample // NUM_TASKS)\n",
    "                         for _ in range(NUM_TASKS)]\n",
    "boot_sample_means_parallel = np.concatenate(ray.get(boot_sample_means_ids))\n",
    "### END SOLUTION\n",
    "\n",
    "bootstrap_parallel_time = time.perf_counter() - start_time\n",
    "print(\"Bootstrap parallel completed in {:.2f} seconds\".format(bootstrap_parallel_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-647fa1a4d19b3c89",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Try restart and run all cells above if this fails\n",
    "assert bootstrap_parallel_time < bootstrap_serial_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52447fecff96812f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now let's examine the resulting distributions. Do they look similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b421389b46c2b59",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Serial Bootstrap Distribution\")\n",
    "plt.xlabel(r\"$\\bar{X}$\")\n",
    "plt.ylabel(\"Frequency\");\n",
    "sns.distplot(boot_sample_means_serial);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65a4ba7e8f9feb6f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Parallel Bootstrap Distribution\")\n",
    "plt.xlabel(r\"$\\bar{X}$\")\n",
    "plt.ylabel(\"Frequency\");\n",
    "sns.distplot(boot_sample_means_parallel);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-214612ee2e95e181",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "At a high level, you parallelized a slow piece of code by splitting it into many tasks, and then *reduced* the result of the individual tasks to produce a output identical to the slow code.\n",
    "\n",
    "In addition, you specifically parallelized the `for` loop in `bootstrap_serial` to speed up bootstrap. Parallelizing loops can often speed up code which allows data scientists to process data more quickly.\n",
    "\n",
    "This idea of parallelizing certain code and reducing the results led to a more general programming model used to reason about and implement parallel programs called MapReduce. We'll introduce and implement MapReduce in Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9793d86189e615e7",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3: Implementing MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-940d8be22eacf849",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) a computational pattern for computing aggregate statistics of large datasets. It is the core primitive in systems like MapReduce, Hadoop, and Spark.\n",
    "\n",
    "At its core, MapReduce consists of two primitives:\n",
    "\n",
    "- The **map** transformation takes a dataset and a function and applies the function to each data point.\n",
    "- The **reduce** transformation aggregates the output of the map stage.\n",
    "\n",
    "For example, suppose that our starting point is a collection of documents. If we wish to count the number of occurrences of each word in the document, we can first apply a \"map\" transformation, which turns each document into a dictionary mapping words to the number of occurrences within that document. Then we can apply the \"reduce\" transformation, which sums the counts for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3a\n",
    "Implement `map_parallel` using the Ray API. We have provided a `map_serial` implementation for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d7843be01131df0",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def map_serial(function, element_lst):\n",
    "    \"\"\"Apply a function to each element of a list\n",
    "    Args:\n",
    "        function: a function that takes in one argument as input and outputs a value\n",
    "        element_lst: a list of elements that function will be applied to\n",
    "    Returns:\n",
    "        A list of all of the elements each transformed by the function\n",
    "        (ie [function(elem_1), function(elem_2), ..., function(elem_n)])\n",
    "    \"\"\"\n",
    "    return [function(elem) for elem in element_lst]\n",
    "\n",
    "\n",
    "map_serial(lambda x: x * x, [1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def map_parallel(function, arglist):\n",
    "    \"\"\"Apply a function to each element of a list in parallel.\n",
    "    Args:\n",
    "        function: a remote function that takes in one argument as input and outputs an ObjectID\n",
    "        arglist: a list of arguments that the function will be applied to\n",
    "    Returns:\n",
    "        A list of ObjectIDs\n",
    "    \"\"\"\n",
    "    if not isinstance(arglist, list):\n",
    "        raise ValueError(\"The arglist argument must be a list.\")\n",
    "    \n",
    "    if not hasattr(function, \"remote\"):\n",
    "        raise ValueError(\"The function argument must be a remote function.\")\n",
    "            \n",
    "    ### BEGIN SOLUTION\n",
    "    return [function.remote(arg) for arg in arglist]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2a-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check, this should pass if you implemented everything correctly\n",
    "@ray.remote\n",
    "def square_remote(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "arglist = [1, 2, 3, 4, 5]\n",
    "\n",
    "with timeit('parallel'):\n",
    "    result_ids = map_parallel(square_remote, arglist)\n",
    "\n",
    "assert isinstance(result_ids[0], ray.ObjectID), \"map_parallel should return a list of ObjectIDs\"\n",
    "\n",
    "result_ray = ray.get(result_ids)\n",
    "\n",
    "with timeit('serial'):\n",
    "    result_serial = map_serial(lambda x: x * x, arglist)\n",
    "\n",
    "assert result_ray == result_serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3326f9482c5ee91d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "You may notice that for a simple function like square operating on a small list of arguments, `map_parallel` is slower than `map_serial`. The difference in speed is due to communication and data transfer overhead between the processes which Ray uses to run functions in parallel.\n",
    "\n",
    "In general, functions that are called often and take a long time to run are good targets for parallelization. We'll explore such an example using some of Shakespeare's works in later questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3b\n",
    "Implement `reduce_parallel` using the Ray API. We have provided a `reduce_serial` implementation for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e10a5ebc075fea7",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_serial(function, items):\n",
    "    \"\"\"Apply a function repeatedly to pairs of items until only 1 remains\n",
    "    \n",
    "    Args:\n",
    "        function: remote function that takes 2 items as input and returns 1 new item.\n",
    "        items: a list of items which are reduced to 1 output by repeatedly calling function.\n",
    "    \n",
    "    Example:\n",
    "    ```\n",
    "    >>> reduce_serial(sum, [1,2,3])\n",
    "    6\n",
    "    >>> reduce_serial(lambda x, y: x - y, [3,2,1])\n",
    "    0\n",
    "    ```\n",
    "    \n",
    "    Returns the resulting item.\n",
    "    \"\"\"\n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    \n",
    "    result = items[0]\n",
    "    for i in range(1, len(items)):\n",
    "        result = function(result, items[i])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def reduce_parallel(function, items):\n",
    "    \"\"\"Apply a function repeatedly to pairs of items until only 1 remains.\n",
    "    \n",
    "    Args:\n",
    "        function: remote function that takes 2 items as input and returns 1 new item.\n",
    "        items: a list of items which are reduced to 1 output by repeatedly calling function.\n",
    "    \n",
    "    Returns an ObjectID.\n",
    "    \n",
    "    Hint:\n",
    "        1. Divide the list of items into pairs.\n",
    "        2. Reduce each pair to generate a new list of items.\n",
    "        3. If there was an unpaired item in (1), add it to the new list.\n",
    "        The new list should be about 1/2 the size of the old list.\n",
    "        4. If there is only 1 item in the new list, return that item. Otherwise, repeat steps 1-3.\n",
    "        \n",
    "        This algorithm is called a \"tree-reduce\", where the original items are the leaves\n",
    "        and the final result is the root. The tree is balanced. Each non-leaf node has 2 child nodes.\n",
    "        Each non-root node has 1 parent node.\n",
    "    \"\"\"\n",
    "    if not isinstance(items, list):\n",
    "        raise ValueError(\"The items argument must be a list.\")\n",
    "\n",
    "    if not hasattr(function, \"remote\"):\n",
    "        raise ValueError(\"The function argument must be a remote function.\")\n",
    "       \n",
    "    items = items.copy()   # Avoids mutating the items argument\n",
    "        \n",
    "    ### BEGIN SOLUTION\n",
    "    if len(items) == 1:\n",
    "        return ray.put(items[0])\n",
    "    \n",
    "    while len(items) > 1:\n",
    "        a = items.pop(0)\n",
    "        b = items.pop(0)\n",
    "        \n",
    "        oid = function.remote(a, b)\n",
    "        items.append(oid)\n",
    "        \n",
    "    return items[0]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2b-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def add_normal(a, b):\n",
    "    # Simulate a longer running function\n",
    "    # Necessary to check the correct implementation for reduce_parallel\n",
    "    time.sleep(0.3)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def add_remote(a, b):\n",
    "    # Simulate a longer running function\n",
    "    # Necessary to check the correct implementation for reduce_parallel\n",
    "    time.sleep(0.3)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "start_time = time.time()\n",
    "result_serial = reduce_serial(add_normal, items)\n",
    "time_serial = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "result_id = reduce_parallel(add_remote, items)\n",
    "result_parallel = ray.get(result_id)\n",
    "time_parallel = time.time() - start_time\n",
    "\n",
    "\n",
    "assert result_serial == result_parallel\n",
    "\n",
    "assert time_serial > time_parallel, \"reduce_parallel is slower than reduce_serial\" \n",
    "\n",
    "assert time_serial > 2 * time_parallel, \"reduce_parallel is too slow\" \n",
    "\n",
    "print(f\"Time serial: {time_serial}\")\n",
    "print(f\"Time parallel: {time_parallel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16ef564de43de85a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 4: Analyzing Word Frequency using MapReduce\n",
    "Let's analyze the frequency of words used in Shakespeare. For this question, you'll need to do the following:\n",
    "1. Download the text of a few of Shakespeare's plays.\n",
    "2. For each play, count the number of times each word occurs.\n",
    "3. Merge the word frequencies across plays.\n",
    "\n",
    "We've provided some helper functions that will help you with this problem below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52a8f14adcbac113",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def download_text(url):\n",
    "    \"\"\"Returns the string of text on some webpage\"\"\"\n",
    "    request = urllib.request.urlopen(url)\n",
    "    return request.read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def download_text_remote(url):\n",
    "    return download_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f61d41bd8d4dbcad",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Finds the frequency of each word in a string\"\"\"\n",
    "    assert isinstance(text, str), \"text should be a string\"\n",
    "    \n",
    "    frequency = dict()\n",
    "    \n",
    "    text = text.lower()\n",
    "    matches = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    \n",
    "    for word in matches:\n",
    "        count = frequency.get(word, 0)\n",
    "        frequency[word] = count + 1\n",
    "        \n",
    "    return frequency\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def count_words_remote(text):\n",
    "    return count_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-511696b791d74ac9",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def merge_dicts(a, b):\n",
    "    \"\"\"Merges 2 dictionaries such that the result contains keys of both a and b.\n",
    "    \n",
    "    If a key k is in a and in b, result[k] = a[k] + b[k].\n",
    "    \"\"\"\n",
    "    result = a.copy()  # Don't mutate the input dictionaries\n",
    "    for key, value in b.items():\n",
    "        result[key] = result.get(key, 0) + value\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def merge_dicts_remote(a, b):\n",
    "    return merge_dicts(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a85664ec269c1dea",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.gutenberg.org/files/1524/1524-0.txt\",       # Hamlet\n",
    "    \"https://www.gutenberg.org/cache/epub/2264/pg2264.txt\",  # Macbeth\n",
    "    \"https://www.gutenberg.org/cache/epub/2267/pg2267.txt\",  # Othello\n",
    "    \"https://www.gutenberg.org/cache/epub/1777/pg1777.txt\",  # Romeo and Juliet\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-308a358634cd8379",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4\n",
    "Speed up the code in the cell below using the MapReduce API. You'll need to use both `map_parallel` and `reduce_parallel`.\n",
    "\n",
    "**HINT 1:** Parallelizing loops often speeds up code.\n",
    "\n",
    "**HINT 2:** If you did question 3 correctly, you should be able to write 1 line solutions for the `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe33c7f16a5260e5",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Parallelize the following code using the MapReduce API\n",
    "start_time = time.time()\n",
    "\n",
    "total_frequencies_serial = {}\n",
    "for url in urls:\n",
    "    # Download the text of the play\n",
    "    text = download_text(url)\n",
    "    # Count the frequency of each word in the play\n",
    "    frequencies = count_words(text)\n",
    "    # Add the play's word frequencies to the global word frequencies\n",
    "    total_frequencies_serial = merge_dicts(total_frequencies_serial, frequencies)\n",
    "    \n",
    "word_freq_time_serial = time.time() - start_time\n",
    "print(\"Word frequency count serial completed in {} seconds\".format(word_freq_time_serial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Try to do these in one line!\n",
    "text_ids = ...\n",
    "frequency_ids = ...\n",
    "total_frequency_id = ...\n",
    "total_frequencies_ray = ...\n",
    "### BEGIN SOLUTION\n",
    "text_ids = map_parallel(download_text_remote, urls)\n",
    "frequency_ids = map_parallel(count_words_remote, text_ids)\n",
    "total_frequency_id = reduce_parallel(merge_dicts_remote, frequency_ids)\n",
    "total_frequencies_ray = ray.get(total_frequency_id)\n",
    "### END SOLUTION\n",
    "\n",
    "word_freq_time_parallel = time.time() - start_time\n",
    "print(\"Word frequency count parallel completed in {} seconds\".format(word_freq_time_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q3-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert total_frequencies_ray == total_frequencies_serial\n",
    "assert word_freq_time_parallel < word_freq_time_serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9d9c250e73b7c65",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's examine some of Shakespeare's most used words. Surprised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8dfded19a18be090",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def most_used_words(freq_dict, num_words=10):\n",
    "    ordered_keys = sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "    return [word for _, word in zip(range(num_words), ordered_keys)]\n",
    "\n",
    "most_used_words(total_frequencies_ray, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc4cf6d789ac7ed2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 5: Ray Actors\n",
    "So far, we have explored Ray's *remote functions*. The tasks generated by invoking remote functions are stateless in the sense that they are intended to map inputs to outputs without side effects. But suppose we want state to be shared and mutated by multiple tasks. In this case, we can use [Ray's *actors*](https://ray.readthedocs.io/en/latest/actors.html) to encapsulate mutable state.\n",
    "\n",
    "*Optional:* Take a look at one interesting example of how to implement [distributed training with a parameter server using Ray actors](https://ray-project.github.io/2018/07/15/parameter-server-in-fifteen-lines.html). \n",
    "\n",
    "To create an actor, we decorate a Python class with the `@ray.remote` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd84d64d0ccc1729",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "    \n",
    "    def increment(self):\n",
    "        self.value += 1\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0d6b2dd965571da",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We can create an actor instance by invoking `.remote()` on the actor class. This starts a new actor process, which holds a copy of the `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0481ce1a9e5aba1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "c = Counter.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-732ef8ed0b021f9a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We can run tasks on the actor process by invoking the actor's methods. These methods can mutate the actor's internal state (in this case, the field `self.value`). The actor executes tasks serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67a29b4825d539d7",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x1_id = c.increment.remote()\n",
    "print(\"The actor's value is {}.\".format(ray.get(c.get_value.remote())))\n",
    "\n",
    "x2_id = c.increment.remote()\n",
    "print(\"The actor's value is {}.\".format(ray.get(c.get_value.remote())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df8a1f4ff42d8e84",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Suppose we want multiple tasks, actors, or processes to invoke methods on a single actor. In this case, we can pass *actor handles* around between tasks. In the example below, we pass a handle to the counter actor to a handful of tasks executing in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-217811ba520bb99b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def increment_counter(c):\n",
    "    for _ in range(10):\n",
    "        x_id = c.increment.remote()\n",
    "\n",
    "    # Wait for the last increment call to complete before returning.\n",
    "    ray.get(x_id)\n",
    "\n",
    "\n",
    "initial_value = ray.get(c.get_value.remote())\n",
    "\n",
    "# Start 4 tasks that run in parallel and all increment the counter.\n",
    "increment_results = [increment_counter.remote(c) for _ in range(4)]\n",
    "\n",
    "# Wait for all tasks to finish\n",
    "ray.get(increment_results)\n",
    "\n",
    "new_value = ray.get(c.get_value.remote())\n",
    "\n",
    "print(\"The actor's value was {} and now it is {}.\".format(initial_value, new_value))\n",
    "assert new_value - initial_value == 4 * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bb141de5cf51759",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5\n",
    "This question will take a different approach to solving the word-count problem from question 3.\n",
    "\n",
    "Instead of launching four tasks that compute word counts and then aggregating the results on the \"driver\" process that issued the tasks, we are going to create a separate `ResultAggregator` actor for doing the aggregation. We will start four tasks that each compute frequencies for a given URL and then push those frequencies to the aggregator. The main \"driver\" process will then fetch the aggregated results from the aggregator.\n",
    "\n",
    "The code below implements a serial version of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "awefawefaewf",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class ResultAggregator(object):\n",
    "    \"\"\"Aggregates word frequencies\"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_frequencies = {}\n",
    "    \n",
    "    def add_frequencies(self, frequencies):\n",
    "        \"\"\"Adds a new dictionary mapping words to word frequencies to the overall word frequencies\"\"\"\n",
    "        self.total_frequencies = merge_dicts(self.total_frequencies, frequencies)\n",
    "    \n",
    "    def get_frequencies(self):\n",
    "        \"\"\"Returns a dictionary mapping each word to its frequency\"\"\"\n",
    "        return self.total_frequencies\n",
    "\n",
    "\n",
    "def add_results(url, result_aggregator):\n",
    "    \"\"\"Downloads text from the url, counts the word frequencies, and adds the result to result_aggregator\"\"\"\n",
    "    # Download the text of the play\n",
    "    text = download_text(url)\n",
    "    # Count the frequency of each word in the play\n",
    "    frequencies = count_words(text)\n",
    "    # Add the results to the aggregator\n",
    "    done = result_aggregator.add_frequencies(frequencies)\n",
    "\n",
    "\n",
    "result_aggregator = ResultAggregator()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "get_and_add_frequencies = [add_results(url, result_aggregator) for url in urls]\n",
    "# ray.get(get_and_add_frequencies)\n",
    "\n",
    "# Get the results\n",
    "total_frequencies = result_aggregator.get_frequencies()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Counting the words took {} seconds.\".format(end_time - start_time))\n",
    "\n",
    "most_used_words(total_frequencies, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e034af828b8433e9",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now implement a parallel version in which `ResultAggregator` is an actor, and `add_results` is a remote function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class ResultAggregator(object):\n",
    "    \"\"\"Aggregates word frequencies\"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_frequencies = {}\n",
    "    \n",
    "    def add_frequencies(self, frequencies):\n",
    "        \"\"\"Adds a new dictionary mapping words to word frequencies to the overall word frequencies\"\"\"\n",
    "        self.total_frequencies = merge_dicts(self.total_frequencies, frequencies)\n",
    "    \n",
    "    def get_frequencies(self):\n",
    "        \"\"\"Returns a dictionary mapping each word to its frequency\"\"\"\n",
    "        return self.total_frequencies\n",
    "\n",
    "@ray.remote\n",
    "def add_results(url, result_aggregator):\n",
    "    \"\"\"Downloads text from the url, counts the word frequencies, and adds the result to result_aggregator\"\"\"\n",
    "    # Download the text of the play\n",
    "    text = download_text(url)\n",
    "    # Count the frequency of each word in the play\n",
    "    frequencies = count_words(text)\n",
    "    # Add the results to the aggregator\n",
    "    done = ...\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create an instance of a ResultAggregator actor\n",
    "result_aggregator = ...\n",
    "\n",
    "# Call the add_results remote function on each URL\n",
    "get_and_add_frequencies = ...\n",
    "\n",
    "# Wait for all add_results remote functions to complete\n",
    "# uncomment this when get_add_frequencies is defined\n",
    "# ray.get(get_and_add_frequencies)\n",
    "\n",
    "# Get the results from the actor\n",
    "total_frequencies = ...\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "@ray.remote\n",
    "def add_results(url, result_aggregator):\n",
    "    \"\"\"Downloads text from the url, counts the word frequencies, and adds the result to result_aggregator\"\"\"\n",
    "    # Download the text of the play\n",
    "    text = download_text(url)\n",
    "    # Count the frequency of each word in the play\n",
    "    frequencies = count_words(text)\n",
    "    # Add the results to the aggregator\n",
    "    done = result_aggregator.add_frequencies.remote(frequencies)\n",
    "    \n",
    "result_aggregator = ResultAggregator.remote()\n",
    "get_and_add_frequencies = [add_results.remote(url, result_aggregator) for url in urls]\n",
    "ray.get(get_and_add_frequencies)\n",
    "total_frequencies = ray.get(result_aggregator.get_frequencies.remote())\n",
    "### END SOLUTION\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Counting the words took {} seconds.\".format(end_time - start_time))\n",
    "\n",
    "most_used_words(total_frequencies, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34efd33520eaa735",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 6: Modin\n",
    "\n",
    "Now that you have learned how to parallelize your applications on a low level, let's take a look at a library that uses Ray to parallelize data science workflows. \n",
    "\n",
    "`Modin` is a DataFrame library that allows you to speed up your pandas workflows _by changing one line of code_. One of the main challenges with pandas is that it can be slow because it is only using a single CPU core on your machine. With Ray, Modin uses all the cores on your machine to make running pandas code faster!\n",
    "\n",
    "To use Modin, replace the following line in your pandas notebook:\n",
    "\n",
    "```python\n",
    "# import pandas as pd\n",
    "import modin.pandas as pd\n",
    "```\n",
    "\n",
    "After this, if you do not have a Ray instance started, Modin will automatically start one. In this notebook, we have used Ray before, so Modin will automatically connect to the Ray instance we started above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-199d343011d1e93c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 6\n",
    "\n",
    "This question relates to using Modin instead of pandas on your existing workflow.\n",
    "\n",
    "We have already learned about pandas and how it can be used in different ways. Suppose we have already built a notebook to do some analysis in pandas. Change the following code to use Modin instead of pandas, and complete the analysis.\n",
    "\n",
    "Note: Do not worry if you see a ray.init warning after running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d3e4703ed1ca1769",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Modin pandas version\n",
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(21919)\n",
    "\n",
    "frame_data = np.random.randint(0, 100, size=(2**19, 2**6))\n",
    "df = pd.DataFrame(frame_data).add_prefix(\"col_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28af8960357cbf98",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Perform a sum on the DataFrame object\n",
    "sums = ...\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "sums = df.sum()\n",
    "### END SOLUTION\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"In Modin, sum on {} rows took {} seconds.\".format(len(df.index), end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cfddb234d59ed657",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Regular pandas version\n",
    "import pandas\n",
    "pandas_df = pandas.DataFrame(frame_data).add_prefix(\"col_\")\n",
    "start_time = time.time()\n",
    "pandas_sums = pandas_df.sum()\n",
    "end_time = time.time()\n",
    "print(\"In pandas, sum on {} rows took {} seconds.\".format(len(df.index), end_time - start_time))\n",
    "assert pandas_sums.equals(sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25b37b1800e6de61",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Optional: Distributed K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d5448c930e109b3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's parallelize an algorithm designed to find clusters of data.\n",
    "\n",
    "[*K-means*](https://en.wikipedia.org/wiki/K-means_clustering) is an unsupervised algorithm that partitions $n$ data points into $k$ clusters where each data point belongs to the cluster with the nearest mean data point or *centroid*.\n",
    "\n",
    "The algorithm works as follows:\n",
    "1. Initialize the centroids $c_1, \\dots, c_k$ randomly.\n",
    "2. Assign each data point $d_j$ to the cluster belonging to the closest centroid. In mathematical terms, generate sets $S_1, \\dots S_k$ s.t.\n",
    "$S_i = \\{d_j : || d_j - c_i || < || d_j - c_l || \\quad \\forall j, k \\quad 1 \\leq j \\leq n \\quad 1 \\leq l \\leq k \\}$\n",
    "3. Calculate $c_1', \\dots, c_k'$ s.t. $c_i' \\leftarrow \\text{mean}(S_i)$.\n",
    "4. If none of the centroids change, finish! In other words, $c_1 = c_1' \\cap \\dots \\cap c_k = c_k'$. \n",
    "Then, $c_1', \\dots, c_k'$ are the centroids of the clusters defined by $S_1, \\dots S_k$.\n",
    "5. Otherwise, assign $c_1 \\leftarrow c_1', \\dots, c_k \\leftarrow c_k'$.\n",
    "6. Return to step 2.\n",
    "\n",
    "For our purposes, we'll make a small modification to the algorithm. Instead of assigning each data point to a set \n",
    "$S_i$ and then computing $c_i' \\leftarrow \\text{mean}(S_i)$, we'll only store the number of data points and the sum of the data points in each cluster.\n",
    "This allows us to compute the mean centroid because:\n",
    "\n",
    "$$\\text{mean}(S_i) = \\frac{\\text{sum}(S_i)}{|S_i|}$$\n",
    "\n",
    "$|S_i|$ corresponds to the number of data points in cluster $i$. $\\text{sum}(S_i) = \\sum_{d \\in S_i} d$ corresponds to the sum of the data points in cluster $i$.\n",
    "\n",
    "\n",
    "Here's some additional explanations that you might find useful:\n",
    "- [K-means on Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "- [Stanford CS221 on K-means](http://stanford.edu/~cpiech/cs221/handouts/kmeans.html)\n",
    "- [Visualing K-means](http://tech.nitoyon.com/en/blog/2013/11/07/k-means/)\n",
    "\n",
    "\n",
    "Familliarize yourself with the non-parallel k-means implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e00b46d5057a7cbf",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_centroid():\n",
    "    \"\"\"Generates a random centroid\"\"\"\n",
    "    return np.random.random(size=2)\n",
    "\n",
    "\n",
    "def generate_samples(num_samples, num_clusters, std=0.1):\n",
    "    \"\"\"Generates samples for some number of cluster according to a normal distribution\"\"\"\n",
    "    centroids = [generate_centroid() for _ in range(num_clusters)]\n",
    "    \n",
    "    samples = []\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        current_num_samples = num_samples // num_clusters\n",
    "        if i == 0:\n",
    "            current_num_samples += num_samples % num_clusters\n",
    "        \n",
    "        generated_samples = np.random.normal(centroid, std, (current_num_samples, 2))\n",
    "        samples.append(generated_samples)\n",
    "        \n",
    "    samples = np.concatenate(samples)\n",
    "    np.random.shuffle(samples)  # Occurs in-place\n",
    "    return samples\n",
    "\n",
    "NUM_CLUSTERS = 4\n",
    "NUM_SAMPLES = 1000\n",
    "samples = generate_samples(NUM_SAMPLES, NUM_CLUSTERS, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d3fd61fbde92840",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def k_means_serial(samples, num_clusters):\n",
    "    \"\"\"Finds num_clusters centroids for the samples using k-means\"\"\"\n",
    "    np.random.seed(0)  # Generate centroids in a predictable way so k-means runs deterministically\n",
    "    centroids = [generate_centroid() for _ in range(num_clusters)]\n",
    "    \n",
    "    old_centroids = []\n",
    "    while not np.array_equal(centroids, old_centroids):\n",
    "        sums = [0 for _ in range(num_clusters)]\n",
    "        counts = [0 for _ in range(num_clusters)]\n",
    "        # Classify the samples\n",
    "        for sample in samples:\n",
    "            cluster = 0\n",
    "            min_dist = np.linalg.norm(sample - centroids[0])\n",
    "            \n",
    "            for i, centroid in zip(range(1, num_clusters), centroids[1:]):\n",
    "                dist = np.linalg.norm(sample - centroid)\n",
    "                if dist < min_dist:\n",
    "                    cluster = i\n",
    "                    min_dist = dist\n",
    "            # Record the sums and counts to later compute the new centroids\n",
    "            sums[cluster] += sample\n",
    "            counts[cluster] += 1\n",
    "        \n",
    "        old_centroids = centroids\n",
    "        centroids = [s / count if count > 0 else generate_centroid() for s, count in zip(sums, counts)]\n",
    "                        \n",
    "    np.random.seed(None)  # Reset random seed\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-48c99b2547132f17",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "serial_centroids = k_means_serial(samples, NUM_CLUSTERS)\n",
    "k_means_serial_time = time.time() - start\n",
    "print(\"k_means_serial took {} seconds.\".format(k_means_serial_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2a57b8e4ec44656",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, let's parallelize the k-means algorithm above. Once again, we'll parallelize the `for` loop in `k_means_serial` and parallelize the classification of data points. Classifying a single data point is very fast -- faster than the overhead needed to start a remote function. In this case, parallelizing across individual data points doesn't make sense. Instead, we'll parallelize across *batches* of data points -- larger sets of data points that are still smaller than the overall dataset.\n",
    "\n",
    "**HINT:** use the MapReduce API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "extra-credit-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_remote_batch_classifier(centroids):\n",
    "    \"\"\"Returns a remote function that classifies each data point in a batch\"\"\"\n",
    "    @ray.remote\n",
    "    def classify_remote(samples):\n",
    "        \"\"\"Classifies each data point in the batch.\n",
    "        \n",
    "        Returns a dictionary containing the sum and number of data points in the batch for each cluster.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        \n",
    "        for sample in samples:\n",
    "            cluster = 0            \n",
    "            # YOUR CODE HERE: classify the data point by assigning `cluster` to the cluster number\n",
    "            # that the data point belongs to.\n",
    "            ### BEGIN SOLUTION\n",
    "            min_dist = np.linalg.norm(sample - centroids[0])\n",
    "            for i, centroid in zip(range(1, len(centroids)), centroids[1:]):\n",
    "                dist = np.linalg.norm(sample - centroid)\n",
    "                if dist < min_dist:\n",
    "                    cluster = i\n",
    "                    min_dist = dist\n",
    "            ### END SOLUTION\n",
    "            \n",
    "            sum_key = \"{}_sum\".format(cluster)\n",
    "            count_key = \"{}_count\".format(cluster)\n",
    "            data[sum_key] = data.get(sum_key, 0) + sample\n",
    "            data[count_key] = data.get(count_key, 0) + 1\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    return classify_remote\n",
    "\n",
    "\n",
    "def k_means_parallel(samples, num_clusters):\n",
    "    \"\"\"Finds num_clusters centroids for the samples using parallelize k-means\"\"\"\n",
    "    np.random.seed(0)  # Generate centroids in a predictable way so k-means runs deterministically\n",
    "    centroids = [generate_centroid() for _ in range(num_clusters)]\n",
    "    \n",
    "    num_batches = int(ray.global_state.cluster_resources()[\"CPU\"])\n",
    "    \n",
    "    # YOUR CODE HERE: Divide samples into `num_batches` batches.\n",
    "    # Make sure batch sizes are roughly similar\n",
    "    batches = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    batches = np.array_split(samples, num_batches)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # YOUR CODE HERE: `ray.put` each batch. This eliminates overhead of repeatedly copying\n",
    "    # batches to Ray's object store when calling remote functions on a batch\n",
    "    batch_ids = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    batch_ids = [ray.put(batch) for batch in batches]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    old_centroids = []\n",
    "    while not np.array_equal(centroids, old_centroids):\n",
    "        remote_classifier = make_remote_batch_classifier(centroids)\n",
    "        \n",
    "        # YOUR CODE HERE: Classify each batch in parallel using `remote_classifier`\n",
    "        batch_info_dicts_ids = ...\n",
    "        ### BEGIN SOLUTION\n",
    "        batch_info_dict_ids = map_parallel(remote_classifier, batch_ids)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # YOUR CODE HERE: use `merge_dicts_remote` to merge all batch_dict_ids into 1 dictionary \n",
    "        centroid_info = ...\n",
    "        ### BEGIN SOLUTION\n",
    "        centroid_info = ray.get(reduce_parallel(merge_dicts_remote, batch_info_dict_ids))\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        old_centroids = centroids\n",
    "        centroids = []\n",
    "        for i in range(num_clusters):\n",
    "            sum_key = \"{}_sum\".format(i)\n",
    "            count_key = \"{}_count\".format(i)\n",
    "            cluster_sum = centroid_info.get(sum_key, 0)\n",
    "            cluster_count = centroid_info.get(count_key, 0)\n",
    "            if cluster_count == 0:\n",
    "                new_centroid = generate_centroid()\n",
    "            else:\n",
    "                # YOUR CODE HERE: compute the new centroid\n",
    "                new_centroid = ...\n",
    "                ### BEGIN SOLUTION\n",
    "                new_centroid = cluster_sum / cluster_count\n",
    "                ### END SOLUTION\n",
    "            \n",
    "            centroids.append(new_centroid)\n",
    "            \n",
    "    np.random.seed(None)  # Reset the random seed\n",
    "            \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "extra-credit-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "centroids = k_means_parallel(samples, NUM_CLUSTERS)\n",
    "k_means_parallel_time = time.time() - start\n",
    "print(\"k_means_parallel took {} seconds.\".format(k_means_parallel_time))\n",
    "assert np.allclose(serial_centroids, centroids)  # np.allclose avoids floating point issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-986a8c81c0160d40",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "You may observe that the serial code runs faster than the parallel code. In this case, the slowdown for the parallel code is likely due to overhead in running remote functions and serializing data.\n",
    "\n",
    "Because k-means initializes centroids randomly, the number of loop iterations to converge on the final set of centroids may vary. This means that running k-means multiple times on the same dataset could take different amounts of time. This should not cause any issues on this homework because we set the random seed to deterministically initialize centroids.\n",
    "\n",
    "Let's try k-means on a 5x larger dataset -- 5000 data points instead of 1000. The performance gain from parallelizing k-means should outweigh the overhead and parallel k-means should run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5a943cf6d490a08",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run on a large dataset. This might take up to a minute.\n",
    "NUM_SAMPLES = 5000\n",
    "samples = generate_samples(NUM_SAMPLES, NUM_CLUSTERS, 0.05)\n",
    "\n",
    "start = time.time()\n",
    "serial_centroids = k_means_serial(samples, NUM_CLUSTERS)\n",
    "k_means_serial_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "centroids = k_means_parallel(samples, NUM_CLUSTERS)\n",
    "k_means_parallel_time = time.time() - start\n",
    "\n",
    "print(\"k_means_serial took {} seconds.\".format(k_means_serial_time))\n",
    "print(\"k_means_parallel took {} seconds.\".format(k_means_parallel_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce985d281c0d1874",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The parallel version of K-means should run much faster.\n",
    "\n",
    "Now, let's visualize the cluster centers you found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00ce05d7040ef72d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "voronoi = scipy.spatial.Voronoi(centroids)\n",
    "scipy.spatial.voronoi_plot_2d(voronoi, show_vertices=False, point_size=20)\n",
    "plt.scatter(samples[:, 0], samples[:, 1], alpha=0.1, color=\"orange\")\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Voronoi Diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15e2b642b7fe20d5",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Other Links\n",
    "\n",
    "You've seen above how Ray makes it easy to implement a system like MapReduce as a library on top of Ray. A handful of libraries have been implemented on top of Ray. You may be interested in taking a look at the following:\n",
    "- [*Tune*: Distributed hyperparameter search](https://ray.readthedocs.io/en/latest/tune.html)\n",
    "([github](https://github.com/ray-project/ray/tree/master/python/ray/tune))\n",
    "- [*RLlib*: Scalable reinforcement learning](https://ray.readthedocs.io/en/latest/rllib.html)\n",
    "([github](https://github.com/ray-project/ray/tree/master/python/ray/rllib))\n",
    "- [*Modin*: Speeding up Pandas](https://modin.readthedocs.io/en/latest/)\n",
    "([github](https://github.com/modin-project/modin))\n",
    "\n",
    "Ray, Tune, RLlib, and Modin are research projects in UC Berkeley's RISElab. If you're interested in contributing, take a look at the project's codebase on Github!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
